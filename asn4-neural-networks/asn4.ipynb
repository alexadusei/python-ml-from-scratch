{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "Now that we wrapped out logistic regression for image classification with our hand-written wordset, we hit a classification accuracy of about 97.5%\n",
    "\n",
    "Thats reasonably good, but pretty much maxes out what we can achieve with a linear model. This time, we'll tackle that same problem with neural networks using a feed-forward neural network with backpropagation.\n",
    "\n",
    "We'll implement regularized/unregularized versions of the neural network cost function and compute gradients via the backpropagation algorithm.\n",
    "\n",
    "This isn't for the faint of heart. Best get a cup of coffee for this one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " '__globals__': [],\n",
       " '__header__': 'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ..., \n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "data = loadmat(pwd + '/asn4/data/ex4data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We're also going to need to one-hot encode our labels. One-hot encoding turns a class label n (out of k classes) into a vector of length k where index n is \"hot\" (1) and the rest are zero. Essentially, if we have a 4, then we make the 4th index 1, and all the other indices 0, and so on.\n",
    "\n",
    "Scikit-learn has a built-in utility we can use for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "\n",
    "print(y_onehot)\n",
    "print(y_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_onehot[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The NN we'll build in this exercise has an input layer matching the size of our instance data (400 pixels, so 400 neurons for our input), a hidden layer with 25 units (26 including the bias unit), and an output layer of 10 units, corresponding to the number of classes we have (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Cost Function (Unregularized)\n",
    "\n",
    "The first piece we'll implement is a cost function to evaluate the loss for a given set of network parameters.\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} \\bigg[ -y_k^{(i)}log((h_\\theta(x^{(i)}))_k) - (1 - y_k^{(i)})log(1 - (h_\\theta(x^{(i)}))_k) \\bigg] $$\n",
    "\n",
    "We have the same cost function as before, just with a few tweaks. The main addition here is the K summation. This is; we are doing the cost function, but for *every class*\n",
    "\n",
    "Remember, the $h_\\theta(x^{(i)})$ just means *our hypothesis that results in the kth output*, and we compare this to $ y_k $ (our answer which is for the kth output), all for each training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Now we'll write our forward propagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# we have two theta matrices for each layer. Theta1 is for the input\n",
    "# layer to calculate the hidden layer, and Theta2 is for the hidden layer\n",
    "# to calculate the output layer\n",
    "def forward_propagate(X, theta1, theta2):\n",
    "    # theta1 = (25, 401)\n",
    "    # theta2 = (10, 26)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # add our bias unit to each training example (layer 1)\n",
    "    a1 = np.insert(X, 0, values=np.ones(m), axis=1) # (5000, 401)\n",
    "    \n",
    "    # we combine our input and theta\n",
    "    z2 = a1 * theta1.T # (5000, 25)\n",
    "    \n",
    "    # add our bias unit to each trainnig example (layer 2)\n",
    "    # we activate our input and theta (which is z2), activating it is\n",
    "    # running it under the sigmoid function, converting z2 to a2\n",
    "    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1) # (5000, 26)\n",
    "    \n",
    "    # we combine our hidden layer with theta, creating z3\n",
    "    z3 = a2 * theta2.T # (5000, 10)\n",
    "    \n",
    "    # final hypothesis, by activating z3 via sigmoid()\n",
    "    h = sigmoid(z3) #(5000, 10)\n",
    "    \n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def cost(theta_params, input_size, hidden_size, num_labels, X, y, reg_lambda):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # reshape the parameter array into parameter matrices for each layer\n",
    "    # it is our hidden layer size * input layer size because each hidden\n",
    "    # layer node is connected to all 400 input nodes, for each hidden\n",
    "    # node. That's 400 * 25. These Theta parameters are for ALL edges\n",
    "    # from one layer to another\n",
    "    \n",
    "    # (25, 401)\n",
    "    theta1 = np.matrix(np.reshape(theta_params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    \n",
    "    # (10, 26)\n",
    "    theta2 = np.matrix(np.reshape(theta_params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # compute the cost, vectorized approach\n",
    "    # because y and h are matrices representing classes, the only important\n",
    "    # information will be y' * h, which will be a (k x k) matrix (10x10) in\n",
    "    # this case. That means only their diagonals are important. Think;\n",
    "    # \"show me the error rate of y with class k and h with class k\". The\n",
    "    # only time we have the same classes are when we match them for the \n",
    "    # row & col they are in. This means class 1 in y is matched with class 1\n",
    "    # in h, class 2 for y is matched with class 2 for h, etc... So we\n",
    "    # only care about the resulting matrix multiplication of the diagonals\n",
    "    \n",
    "    # we can get just the diagonal values by using trace(), which does just\n",
    "    # that. Much more efficient than using for-loops, like this tutorial does!\n",
    "    J = (1.0 / m) * (np.trace(-y.T * np.log(h)) - np.trace((1 - y).T * np.log(1 - h)))\n",
    "    return J    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll test this to ensure its working properly. Seeing the output\n",
    "# from intermediate steps is helpful to understand what's going on\n",
    "\n",
    "# initial setup\n",
    "input_size = 400\n",
    "hidden_size = 25\n",
    "num_labels = 10\n",
    "reg_lambda = 1\n",
    "\n",
    "# randomly initialize a parameter array of the size of the full network's parameters\n",
    "theta_params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)) - 0.5) * 0.25\n",
    "\n",
    "m = X.shape[0]\n",
    "X = np.matrix(X)\n",
    "y = np.matrix(y)\n",
    "\n",
    "# unroll the parameter array into parameter matrices for each layer\n",
    "theta1 = np.matrix(np.reshape(theta_params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))  \n",
    "theta2 = np.matrix(np.reshape(theta_params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "theta1.shape, theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 401) (5000, 25) (5000, 26) (5000, 10) (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "print a1.shape, z2.shape, a2.shape, z3.shape, h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The cost function, after computing the hypothesis *h*, applies the cost equation to compute the total error between *y* and *h*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "J = cost(theta_params, input_size, hidden_size, num_labels, X, y_onehot, reg_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Cost Function (Regularized)\n",
    "\n",
    "Our next step is adding regularization to our cost function, which adds a penalty term to the cost that scales with the magnitude of the parameters. This is the same as before, just with an added regularization term (looks daunting, but give it a thorough read, it's actually straight forward):\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} \\bigg[ -y_k^{(i)}log((h_\\theta(x^{(i)}))_k) - (1 - y_k^{(i)})log(1 - (h_\\theta(x^{(i)}))_k) \\bigg] + \\frac{\\lambda}{2m} \\Bigg[ \\sum_{j=1}^{25} \\sum_{k=1}^{400} (\\Theta_{j,k}^{(1)})^{2} + \\sum_{j=1}^{10} \\sum_{k=1}^{25} (\\Theta_{j,k}^{(2)})^{2} \\Bigg]$$\n",
    "\n",
    "*Breathe*. All this is saying, like above, is, *for every training example, and for every class, calculate the cost with respect to that class*. On top of that, we are saying *for every theta parameter in theta1, penalize it by a certain amount $\\lambda$, and do the same for every theta parameter in theta2*\n",
    "\n",
    "This balances all the theta parameters we'd use for our feed-forward algorithm, which prevents overfitting. The lambda terms are just for keeping our theta parameters in check. Note that the *k* in the last term's summation have nothing to do with classes, and just represent the size dimensions of our theta parameters, which are hardcoded in this equation for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# we'll just use our previous cost J and add the regularization terms\n",
    "J += (float(reg_lambda) / (2 * m)) * (np.sum(np.power(theta1[:, 1:], 2)) + np.sum(np.power(theta2[:, 1:], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "Now we're ready to implement backpropagation to compute the gradients (this is the learning step). Since the computations required for backprop are superset of those required in the cost function.\n",
    "\n",
    "THIS is the actual complicated step in feedforward neural nets, so we'll break this into 4 parts:\n",
    "\n",
    "1) Run your forward propagation algorithm\n",
    "\n",
    "2) Once you get to the final layer *h*, see how bad you did by comparing your answer for h to our real 'answer-book' y. We call this our delta  layer $\\delta$\n",
    "\n",
    "3) See how bad we did for our hidden layers. We obviously can't compare our hidden layers to y, because they're not based on the final output. Instead, we'll have to \"backtrack\" to figure out our margin of error. Think of it in this scenario: \"We see our margin of error for our output layer by comparing their answers to our answer-book 'y'. We get the differences here. If the differences are extremely small, then our margin of error for our output is small. This means our previous neurons did well. If we have some neurons with a large margin of error, then our previous neurons did poorly. The only thing to blame right now must be the previous neuron(s) that gave this bad neuron its bad answer. So we must penalize those previous neurons.\". In order to penalize them, we let the previous neurons know how bad THEY did by showing them the margin of error we got based on their inputs to us ('us' being the output layer). We share with them how bad we did by multiplying their weights by our $\\delta$ value (our margin of error). This is all multiplied by the derivative of the sigmoid function (we'll define this later). We keep doing this process ONLY up until the last hidden layer, NOT the input layer (therefore, this will be layer 2)\n",
    "\n",
    "$$ \\delta^{(l)} = (\\Theta^{(2)})^T \\delta^{(3)} \\cdot g'(z^{(2)}) $$\n",
    "\n",
    "Where $ g'(z^{(2)}) $ is the derivative of the sigmoid function, defined as:\n",
    "\n",
    "$$ g'(z) = a^{(l)} \\cdot (1 - a^{(l)})$$\n",
    "\n",
    "4) Now we accumulate our margins of error into one big matrix for each layer. We call this the delta accumulator, $\\Delta$. For each layer, it gets our calculated margin of error vector for said vector and multiplies it by our activation layer. We do this from each layer starting with our input layer (layer 1) up until the last hidden layer (left-to-right), which is layer N-1.\n",
    "\n",
    "$$\\Delta_{i,j}^{(l)} = \\Delta_{i,j}^{(l)} + a_{j}^{(l)}\\delta_{i}^{(l + 1)}$$\n",
    "\n",
    "Once we get the delta accumulator layers (just 2 in this case), we average it out with all of our training examples. This becomes derivatives of the cost with respect to $\\Theta$ for each layer.\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\Theta_{i,j}^{(l)}}J(\\Theta) = \\frac{1}{m}\\Delta_{i,j}^{(l)}, \\ if \\ j = 0 $$\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\Theta_{i,j}^{(l)}}J(\\Theta) = \\frac{1}{m}(\\Delta_{i,j}^{(l)} + \\lambda\\Theta_{i,j}^{(l)}), \\ if \\ j \\neq 0 $$\n",
    "\n",
    "You'd rename this J value as theta1 and theta2, to represent an update of their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll combine our cost function and our backprop into one function just to represent what's happening better (explained above)\n",
    "\n",
    "First, let's define the derivative of the sigmoid function. We'll call this *sigmoid_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return np.multiply(sigmoid(z), (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def backprop(theta_params, input_size, hidden_size, num_labels, X, y, reg_lambda):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "        \n",
    "    # (25, 401)\n",
    "    theta1 = np.matrix(np.reshape(theta_params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    # (10, 26)\n",
    "    theta2 = np.matrix(np.reshape(theta_params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # initializations\n",
    "    J = 0\n",
    "    delta_accumulator1 = np.zeros(theta1.shape) # (25, 401)\n",
    "    delta_accumulator2 = np.zeros(theta2.shape) # (10, 26)\n",
    "    \n",
    "    # compute the cost\n",
    "    J = (1.0 / m) * (np.trace(-y.T * np.log(h)) - np.trace((1 - y).T * np.log(1 - h)))\n",
    "    \n",
    "    # add cost regularization term\n",
    "    J += (float(reg_lambda) / (2 * m)) * (np.sum(np.power(theta1[:, 1:], 2)) + np.sum(np.power(theta2[:, 1:], 2)))\n",
    "    \n",
    "    ### end of cost function, now on to backprop ###\n",
    "    for t in range(m):\n",
    "        # Part 1: run through forwardprop\n",
    "        a1t = a1[t, :] # (1, 401)\n",
    "        z2t = z2[t, :] # (1, 25)\n",
    "        a2t = a2[t, :] # (1, 26)\n",
    "        ht = h[t, :]   # (1, 10)\n",
    "        yt = y[t, :]   # (1, 10)\n",
    "        \n",
    "        # Part 2: Get delta for layer 3 by seeing how bad we did\n",
    "        d3t = ht - yt # (1, 10)\n",
    "        \n",
    "        # Part 3: Get deltas for hidden layer (layer 2)\n",
    "        # REMEMBER: np.multiply is element-wise multiplication, while\n",
    "        # * is matrix multiplication\n",
    "        z2t = np.insert(z2t, 0, values=np.ones(1)) # (1, 26)\n",
    "        d2t = np.multiply((theta2.T * d3t.T).T, sigmoid_gradient(z2t)) # (1, 26)\n",
    "        \n",
    "        # Part 4: Accumulate Deltas for margin of errors into Delta accumulators\n",
    "        # we'll get an average of our accumulator for all training examples\n",
    "        # afterward by dividing it by m\n",
    "        delta_accumulator1 = delta_accumulator1 + (d2t[:, 1:]).T * a1t\n",
    "        delta_accumulator2 = delta_accumulator2 + d3t.T * a2t\n",
    "        \n",
    "    delta_accumulator1 = delta_accumulator1 / m\n",
    "    delta_accumulator2 = delta_accumulator2 / m\n",
    "    \n",
    "    # add the gradient regularization term (the /m is here again because we didn't\n",
    "    # average out the reg term as well)\n",
    "    delta_accumulator1[:, 1:] = delta_accumulator1[:, 1:] + (theta1[:, 1:] * reg_lambda) / m\n",
    "    delta_accumulator2[:, 1:] = delta_accumulator2[:, 1:] + (theta2[:, 1:] * reg_lambda) / m\n",
    "        \n",
    "    # unroll the gradient matrices into a single array\n",
    "    grad = np.concatenate((np.ravel(delta_accumulator1), np.ravel(delta_accumulator2)))\n",
    "    \n",
    "    return J, grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now let's test it out to make sure the function returns what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.7739154702005706, (10285,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, grad = backprop(theta_params, input_size, hidden_size, num_labels, X, y_onehot, reg_lambda)\n",
    "J, grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now we're ready to train our network and use it to make predictions. This is similay to the previous exercise with multi-class regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done:   status: 3\n",
      " success: False\n",
      "    nfev: 250\n",
      "     fun: 0.32842875596276927\n",
      "       x: array([ -1.31062744e+00,  -8.40792370e-05,  -1.53053993e-04, ...,\n",
      "        -4.75516368e-01,  -2.98868959e+00,  -5.02640103e+00])\n",
      " message: 'Max. number of function evaluations reached'\n",
      "     jac: array([  4.54561845e-04,  -1.68158474e-08,  -3.06107987e-08, ...,\n",
      "        -8.97972806e-05,   9.84707324e-05,   1.55790889e-04])\n",
      "     nit: 20\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# minimize the objective function\n",
    "fmin = minimize(fun=backprop, x0=theta_params, args=(input_size, hidden_size, num_labels, X, y_onehot, reg_lambda),  \n",
    "                method='TNC', jac=True, options={'maxiter': 250})\n",
    "print \"Done:\", fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We put a bound on the number of iterations since the objective function isn't likely to completely converge. Our total cost has dropped below 0.5 though, so that's a good indicator that the algorithm is working\n",
    "\n",
    "Let's use the theta parameters found to run out forward propagation and get some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10],\n",
       "       [10],\n",
       "       [10],\n",
       "       ..., \n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 9]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.matrix(X)\n",
    "\n",
    "# we have to reshape the output from the optimizer to match the\n",
    "# theta parameter matrix shapes that our network is expecting\n",
    "\n",
    "# (25, 401)\n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))  \n",
    "\n",
    "# (10, 26)\n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we can compute the accuracy to see how well our trained network is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 99.34%\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "\n",
    "print 'accuracy = {0}%'.format(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
